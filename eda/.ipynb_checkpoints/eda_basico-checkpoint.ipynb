{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis exploratorio de datos básico\n",
    "\n",
    "## Datos de la competencia HAHA-2021\n",
    "\n",
    "#### [Julio Waissman Vilanova](julio.waissman@unison.mx)\n",
    "\n",
    "Abril, 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import spacy\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "nlp = spacy.load('es_core_news_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtención de datos y visualización de atributos\n",
    "\n",
    "Los datos vienen ya en un archivo csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/haha_2021_train.csv\")\n",
    "\n",
    "print(f\"Las columnas son: {df.columns}\")\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En `text`se encuentran los documentos a ser procesados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(80 * '=')\n",
    "for texto in df.text.sample(10).values:\n",
    "    print(texto)\n",
    "    print(80 * '=')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De los cuales la mayoría no están considerados como de humor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_humor = Counter(df.is_humor.values)\n",
    "\n",
    "print(\"Cuantos textos son de humor voluntario y cuantos no\")\n",
    "print(is_humor)\n",
    "\n",
    "# Podemos ver si hay datos perdidos en humor_mechanism y humor_target\n",
    "# para los casos donde es de humor y donde no es de humor\n",
    "\n",
    "print(\"\\n\\nAqui vemos que de los textos que no son de humor, todos los valores de mecanismo y target son NaN\")\n",
    "print(f\"Para mecanismo = {Counter(df.humor_mechanism[df.is_humor == 0])}\")\n",
    "print(f\"Para target = {Counter(df.humor_target[df.is_humor == 0])}\")\n",
    "print(f\"Para ratio (aplicando all) = {np.all(pd.isna(df.humor_rating[df.is_humor == 0].values))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nOcurrencias y valores de mecanismos\")\n",
    "mec_dic = dict(Counter(df.humor_mechanism[df.is_humor > 0]))\n",
    "df_mec = pd.DataFrame({\n",
    "    \"Mecanismo\": [mec if pd.notna(mec) else pd.NA for mec in mec_dic.keys()],\n",
    "    \"Ocurrencias\": mec_dic.values()\n",
    "})\n",
    "print(df_mec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nOcurrencias y valores de target (tomando repetciones)\")\n",
    "\n",
    "target_names = set([])\n",
    "target_dict = {}\n",
    "for line in df.humor_target[df.is_humor > 0].unique():\n",
    "    if pd.notna(line):\n",
    "        for valor in line.split(';'):\n",
    "            valor = valor.strip()\n",
    "            target_names.add(valor)\n",
    "            target_dict[valor] = target_dict.get(valor, 0) + 1 \n",
    "    else:\n",
    "        target_names.add(\"NA\")\n",
    "\n",
    "target_count = {name: 0 for name in target_names}\n",
    "for line in df.humor_target[df.is_humor > 0]:\n",
    "    if pd.isna(line):\n",
    "        target_count['NA'] += 1\n",
    "    else:\n",
    "        for name in target_count:\n",
    "            if line.find(name):\n",
    "                target_count[name] += 1\n",
    "df_tar = pd.DataFrame({\n",
    "    \"target\": target_count.keys(),\n",
    "    \"count\": target_count.values(),\n",
    "    \"repetido\": [1 if key == \"NA\" else target_dict[key] for key in target_count.keys()]\n",
    "}).sort_values(\"count\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(df_tar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y el *grado de chistozes* evaluado por 5 jurados tiene la siguiente distribución:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"¿Existe algun valor perdido? {np.any(pd.isna(df.humor_rating[df.is_humor > 0]))}\")\n",
    "\n",
    "plt.hist(df.humor_rating[df.is_humor>0].values, bins=25)\n",
    "plt.xlabel(\"ratings entre 1 y 5\")\n",
    "plt.title(\"Distribución de los ratings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algunas nubes de palabras para divertirse\n",
    "\n",
    "Primero vamos a agregar una columna con el texto tratado con *SpaCy*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['spacy'] = [nlp(text) for text in df.text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spacy.displacy.render(df.spacy.sample(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.displacy.render(df.spacy.sample(5), style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a hacer algunas nubes de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genera_nube(docs, pos_tags = [], tipo=None\"):\n",
    "\n",
    "    newText = \"\"\n",
    "    for doc in docs:\n",
    "        for token in doc:\n",
    "            if not any(\n",
    "                [token.is_stop, token.is_currency,\n",
    "                 token.is_punct, token.is_bracket, token.is_quote,\n",
    "                 token.like_num, token.like_email, token.like_url]):           \n",
    "                if not pos_tags or token.pos_ in pos_tags:\n",
    "                    texto = token.lemma_ if tipo == \"lema\" else token.lower_\n",
    "                    newText = \" \".join((newText, texto))\n",
    "    return WordCloud().generate(newText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = genera_nube(\n",
    "    df.spacy[df.is_humor == 0], \n",
    "    pos_tags = ['ADJ'], \n",
    "    tipo=\"-lema\"\n",
    ")\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = genera_nube(\n",
    "    df.spacy[df.is_humor > 0], \n",
    "    pos_tags = ['ADJ'], \n",
    "    tipo=\"-lema\"\n",
    ")\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = genera_nube(\n",
    "    df.spacy[df.humor_mechanism == 'stereotype'], \n",
    "    pos_tags = ['ADJ'], \n",
    "    tipo='-lema'\n",
    ")\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df[pd.notna(df.humor_target)]\n",
    "\n",
    "wordcloud = genera_nube(\n",
    "    df_temp.spacy[df_temp.humor_target.str.contains(\"women\")] , \n",
    "    pos_tags = ['VERB'], \n",
    "    tipo='lema'\n",
    ")\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tareas a desarrollar\n",
    "\n",
    "Al parecer tenemos 3 tareas a desarrollar para ir integrandolas en un pipeline de *SpaCy*:\n",
    "\n",
    "1. Categorizar todos los textos entre humor y no humor (clasificación binaria)\n",
    "2. Clasificar los textos en mecanismo (textcat con una sola clase por texto, no binaria)\n",
    "3. Clasificar los textos por objetivo (textcat con multiples clases por texto)\n",
    "4. Predecir el *indice de chistosez* (regresión a partir de un texto)\n",
    "\n",
    "\n",
    "Todas las podemos desarrollar en *Spacy* usando los proyectos. Hay que verificar si se hace un proyecto por tarea o si podemos agregar todas las tareas en el mismo proyecto. \n",
    "\n",
    "El único caso que no se si se pueda hacer con *Spacy* es el de regresión. Por otro lado, con lo visto en redes neuronales, tambien podemos aplicarlo a estos problemas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
